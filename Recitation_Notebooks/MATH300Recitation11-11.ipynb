{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH 300: Numerical Analysis I Recitation\n",
    "\n",
    "## Instructor: Liam Doherty\n",
    "\n",
    "## Meeting Time/Place: F 11:00AM, Curtis 453\n",
    "\n",
    "## Office Hours: MRC Hours MR 5p-7p, W 12p-2p, or in Korman 282 by appointment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anybody who wants to turn in paper copies of Homework 2 can do so today.\n",
    "\n",
    "Before I get into problems for the day, there are two things that I would like to revisit. The first is regarding the Lagrange error bounds. Recall from Theorem 3.3 that $\\newline \\newline$\n",
    "$$\n",
    "|f(x) - P(x)| = \\Big|\\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod_{k = 0}^{n}(x - x_{k})\\Big|\n",
    "\\newline\n",
    "$$\n",
    "for some $\\xi \\in (x_{0}, x_{n})$. In class, when we had applied this bound, we had done it over the **whole interval**. What this amounts to is not only maximizing the derivative factor, but also the polynomial that it multiplies, i.e., $\\newline \\newline$\n",
    "$$\n",
    "|f(x) - P(x)| \\leq \\frac{1}{(n + 1)!}\\max_{\\xi \\in (x_{0}, x_{1})}\\Big|f^{(n + 1)}(\\xi)\\Big | \\cdot \\max_{x \\in (x_{0}, x_{n})}\\Big|\\prod_{k = 0}^{n}(x - x_{k})\\Big|.\n",
    "\\newline\n",
    "$$\n",
    "While this bound does indeed capture the maximum of the error of the whole interval, we may only want a point estimate of the error (as is the case in some of the homework problems). In this case, if we wanted an estimate of the error at $x = a$, for example, we would do $\\newline \\newline$\n",
    "$$\n",
    "|f(a) - P(a)| \\leq \\frac{1}{(n + 1)!}\\max_{\\xi \\in (x_{0}, x_{1})}\\Big|f^{(n + 1)}(\\xi)\\Big | \\cdot \\Big|\\prod_{k = 0}^{n}(a - x_{k})\\Big|.\n",
    "\\newline\n",
    "$$\n",
    "Notice that now, we will get a sharper error estimate, but it is **only valid at the point**.\n",
    "\n",
    "The other thing I want to mention is about a few packages that are available in Julia. I'd like to comment that `Dierckx.jl` (that I showed last time) also has the ability to provide integrals of cubic splines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Dierckx\n",
    "x = [0.8, 1.]\n",
    "y = [0.22363362, 0.65809197]\n",
    "spl = Spline1D(x, y, k = 1)\n",
    "integrate(spl, 0.8, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one way to do numerical approximation of integrals, but it is not widely used because of lacking error estimates for not much more (if any) computational efficiency compared to traditional quadrature (which we will see). It's also worth noting that because the spline is only defined between the nodes, you will experience weird behavior if you try to evaluate the spline outside of the interval the nodes contain, so be careful of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(spl(1.))\n",
    "println(spl(100.))\n",
    "println(integrate(spl, 0.8, 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing I want to show is `ForwardDiff`, which is a package that allows you to take derivatives of arbitrary functions using **forward mode automatic differentiation** (may be familiar to those interested in ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "g(x) = sin(x)\n",
    "dg(x) = ForwardDiff.derivative(g, x)\n",
    "\n",
    "println(\"Actual derivative at 1: \", cos(1))\n",
    "println(\"Derivative with ForwardDiff: \", dg(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll get to some problems. We're covering Richardson's extrapolation and basics of quadrature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.2.1b\n",
    "\n",
    "Apply the extrapolation process described in Example 1 to determine $N_{3}(h)$, an approximation to $f'(x_{0})$, for the following function and step size. $\\newline \\newline$\n",
    "$$\n",
    "f(x) = x + e^{x}, \\;\\; x_{0} = 0.0, \\;\\; h = 0.4.\n",
    "\\newline\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "We expect to get something close to $2$, since $\\newline \\newline$\n",
    "$$\n",
    "f'(x_{0}) = 1 + e^{0} = 2.\n",
    "\\newline\n",
    "$$\n",
    "Equation (4.16) in section 4.2 of the textbook tells us that a recursive formula for computing $N_{j}(h)$ is $\\newline \\newline$\n",
    "$$\n",
    "N_{j}(h) = N_{j - 1}\\Big(\\frac{h}{2}\\Big) + \\frac{N_{j - 1}(h/2) - N_{j - 1}(h)}{4^{j - 1} - 1}\n",
    "\\newline\n",
    "$$\n",
    "for each $j = 2, 3, \\dots$, and for $j = 1$, we just use our \"usual\" methods to make a low-order approximation (this particular formula is valid for quadratic schemes). To compute $N_{3}(h)$, we begin by simply computing $N_{1}(h)$ (here we use centered differences to obtain that quadratic order): $\\newline \\newline$\n",
    "$$\n",
    "f'(0.0) \\approx N_{1}(0.4) = \\frac{f(0.4) - f(-0.4)}{0.8} \\approx 2.0269.\n",
    "\\newline\n",
    "$$\n",
    "This is already pretty close to our expected result (notice that it is about $\\mathcal{O}(h^{2}) \\asymp \\mathcal{O}(10^{-2})$ away from our desired solution), but we can do better. To compute $N_{2}(h)$, we first need $N_{1}(h/2)$, which is $\\newline \\newline$\n",
    "$$\n",
    "f'(0.0) \\approx N_{1}(0.2) = \\frac{f(0.2) - f(-0.2)}{0.4} \\approx 2.0067.\n",
    "\\newline\n",
    "$$\n",
    "Then, $\\newline \\newline$\n",
    "$$\n",
    "N_{2}(0.4) = N_{1}(0.2) + \\frac{N_{1}(0.2) - N_{1}(0.4)}{3} \\approx 1.999966.\n",
    "\\newline\n",
    "$$\n",
    "We can see how well this approximation is doing already, but let's continue. We need $N_{2}(h/2)$ to compute $N_{3}(h)$, and to compute $N_{2}(h/2)$, we need $N_{1}(h/4)$: $\\newline \\newline$\n",
    "$$\n",
    "N_{1}(0.1) = \\frac{f(0.1) - f(-0.1)}{0.2} \\approx 2.0017.\n",
    "\\newline\n",
    "$$\n",
    "Now we can compute $N_{2}(h/2) = N_{2}(0.2)$: $\\newline \\newline$\n",
    "$$\n",
    "N_{2}(0.2) = N_{1}(0.1) + \\frac{N_{1}(0.1) - N_{1}(0.2)}{3} \\approx 2.000033.\n",
    "$$\n",
    "Finally, we have what we need to compute $N_{3}(h) = N_{3}(0.4)$: $\\newline \\newline$\n",
    "$$\n",
    "N_{3}(0.4) = N_{2}(0.2) + \\frac{N_{2}(0.2) - N_{2}(0.4)}{15} \\approx 2.000000.\n",
    "\\newline\n",
    "$$\n",
    "We've obtained a very accurate solution at only a small cost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.3.1a\n",
    "\n",
    "Approximate the following integral with the trapezoidal rule. $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx\n",
    "\\newline\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "The book lists the \"trapezoidal rule\" as being $\\newline \\newline$\n",
    "$$\n",
    "\\int_{a}^{b}f(x)dx \\approx \\frac{b - a}{2}(f(a) + f(b)),\n",
    "\\newline\n",
    "$$\n",
    "but we can do better. Indeed, instead of essentially using one interpolant over the whole interval, we can partition the interval, interpolate over each subinterval, and sum the resulting integrals. This produces the following \"generalization\" (which is what most people actually mean when they refer to this scheme in practice): $\\newline \\newline$\n",
    "$$\n",
    "\\int_{a}^{b}f(x)dx \\approx \\frac{h}{2}\\Big(f(a) + 2f(a + h) + 2f(a + 2h) + \\dots + 2f(a + Nh) + f(b)\\Big).\n",
    "\\newline\n",
    "$$\n",
    "Here, we assume uniform spacing, and that there are $N$ interior nodes $a + nh$, for $n \\in \\{1, \\dots, N\\}$.\n",
    "\n",
    "So, instead of doing the boring thing (which would be to make the approximation $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx \\approx \\frac{1 - 0.5}{2}(0.5^{4} + 1^{4}) = 0.25 \\cdot (1.0625) = 0.265625\n",
    "\\newline\n",
    "$$\n",
    "for our answer), we'll split up the interval with $h = 0.1$ to obtain $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx = \\int_{0.5}^{0.6}x^{4}dx + \\int_{0.6}^{0.7}x^{4}dx + \\int_{0.7}^{0.8}x^{4}dx + \\int_{0.8}^{0.9}x^{4}dx + \\int_{0.9}^{1}x^{4}dx.\n",
    "\\newline\n",
    "$$\n",
    "Now, we make the approximation $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\int_{0.5}^{1}x^{4}dx &\\approx \\frac{0.1}{2}(f(0.5) + 2f(0.6) + 2f(0.7) + 2f(0.8) + 2f(0.9) + f(1)) \\\\\n",
    "    &\\approx 0.05(0.0625 + 0.2592 + 0.4802 + 0.8192 + 1.3122 + 1) \\\\\n",
    "    &\\approx 0.1967.\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "Of course, we can integrate by hand to see which is the better approximation: $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx = \\frac{x^{5}}{5}\\Big|_{0.5}^{1} = 0.2 - \\frac{0.5^{5}}{5} = 0.2 - 0.1 \\cdot 0.5^{4} = 0.2 - 0.1 \\cdot 0.0625 = 0.2 - 0.00625 = 0.19375.\n",
    "\\newline\n",
    "$$\n",
    "As expected, the approximation with the finer mesh is closer to the true solution than with the \"one-step\" mesh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.1.29\n",
    "\n",
    "Consider the function $\\newline \\newline$\n",
    "$$\n",
    "e(h) = \\frac{\\epsilon}{h} + \\frac{h^{2}}{6}M,\n",
    "\\newline\n",
    "$$\n",
    "where $M$ is a bound on the third derivative of a function. Show that $e(h)$ has a minimum at $\\sqrt[3]{3\\epsilon/M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "All we need to do is perform the standard optimization techniques from calculus. The derivative is $\\newline \\newline$\n",
    "$$\n",
    "e'(h) = -\\frac{\\epsilon}{h^{2}} + \\frac{h}{3}M,\n",
    "\\newline\n",
    "$$\n",
    "which means that a critical point $h$ must satisfy $\\newline \\newline$\n",
    "$$\n",
    "\\frac{\\epsilon}{h^{2}} = \\frac{h}{3}M.\n",
    "\\newline\n",
    "$$\n",
    "Solving for $h$, we find that $h = \\sqrt[3]{3\\epsilon/M}$ is a critical point. Further, it is indeed a minimum value since $\\newline \\newline$\n",
    "$$\n",
    "e''(h) = \\frac{2\\epsilon}{h^{3}} + \\frac{M}{3} > 0\n",
    "\\newline\n",
    "$$\n",
    "when $\\epsilon$, $h$ and $M$ are positive (of course, this is a reasonable assumption to make).\n",
    "\n",
    "The reason for picking this exercise is obviously not for its mathematical content; rather, what this exercise shows us is that if we have a formula with an error term for a numerical scheme, we can optimize it in this way to find an optimal selection for our step size $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems 4.3.5(a) & 4.3.9(a)\n",
    "\n",
    "The integral we consider is $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx.\n",
    "\\newline\n",
    "$$\n",
    "(This is intentionally chosen as the integral from last week's recitation). Approximate this integral with Simpson's rule and the Midpoint rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "Recall that the exact solution to this integral is $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx = 0.19375.\n",
    "\\newline\n",
    "$$\n",
    "The (non-composite) Simpson's rule makes the approximation $\\newline \\newline$\n",
    "$$\n",
    "\\int_{x_{0}}^{x_{2}}f(x)dx = \\frac{h}{3}[f(x_{0}) + 4f(x_{1}) + f(x_{2})] + \\mathcal{O}(h^{5}).\n",
    "\\newline\n",
    "$$\n",
    "Neglecting the error term gives the approximation $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\int_{0.5}^{1}x^{4}dx &\\approx \\frac{0.25}{3}[(0.5)^{4} + 4(0.75)^{4} + 1^{4}] \\\\\n",
    "    &\\approx 0.19401.\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "This is consistent with our error bounds as well, since $0.25^5 \\approx 0.001$.\n",
    "\n",
    "The Midpoint rule makes a simpler approximation: $\\newline \\newline$\n",
    "$$\n",
    "\\int_{x_{-1}}^{x_{1}}f(x)dx = 2hf(x_{0}) + \\mathcal{O}(h^{3})\n",
    "\\newline\n",
    "$$\n",
    "Applying this to our problem while neglecting the error term gives $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\int_{0.5}^{1}x^{4}dx &\\approx 2(0.25)(0.75)^{4} \\\\\n",
    "    &\\approx 0.1582.\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "It is clear that although there is less computational effort here, the approximation is not as good as the approximation for Simpson's rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.4.16(a)\n",
    "\n",
    "In multivariable calculus and statistics courses it is shown that $\\newline \\newline$\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(1/2)(x/\\sigma)^{2}}dx = 1,\n",
    "\\newline\n",
    "$$\n",
    "for any positive $\\sigma$. The function $\\newline \\newline$\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(1/2)(x/\\sigma)^{2}}\n",
    "\\newline\n",
    "$$\n",
    "is the _normal density function with mean_ $\\mu = 0$ _and standard deviation_ $\\sigma$. The probability that a randomly chosen value described by this distribution lies in $[a, b]$ is given by $\\int_{a}^{b}f(x)dx$. Approximate to within $10^{-5}$ the probability that a randomly chosen value described by this distribution will lie in $[-\\sigma, \\sigma]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "The integral we need to evaluate is $\\newline \\newline$\n",
    "$$\n",
    "\\int_{-\\sigma}^{\\sigma}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(1/2)(x/\\sigma)^{2}}dx.\n",
    "\\newline\n",
    "$$\n",
    "Our error bound is $\\newline \\newline$\n",
    "$$\n",
    "|E(f)| \\leq \\frac{b - a}{180}h^{4}\\max_{\\mu}|f^{(4)}(\\mu)| \\leq \\frac{\\sigma}{90}\\cdot\\frac{3}{\\sigma^{5}\\sqrt{2\\pi}}h^{4} = \\frac{1}{30\\sigma^{4}\\sqrt{2\\pi}}h^{4}.\n",
    "\\newline\n",
    "$$\n",
    "(See the appendix for associated derivative calulations.) This, however, is not useful to us, since the error bound is dependent on $\\sigma$! What we can do, however, is rescale the integral with a substitution so that we are integrating the standard Gaussian PDF (that is, the PDF of a random variable distributed as $\\mathcal{N}(0, 1)$). Let $y = x/\\sigma$. Then the original integral is equivalent to $\\newline \\newline$\n",
    "$$\n",
    "\\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{y^{2}}{2}}dy.\n",
    "\\newline\n",
    "$$\n",
    "From the fruits of our labor in the appendix, setting $\\sigma = 1$ allows us to recover that a bound on the fourth derivative for this standard Gaussian is $\\newline \\newline$\n",
    "$$\n",
    "|f^{(4)}(x)| \\leq \\frac{3}{\\sqrt{2\\pi}}.\n",
    "\\newline\n",
    "$$\n",
    "Then, our overall bound on the error is $\\newline \\newline$\n",
    "$$\n",
    "E(f) \\leq \\frac{1}{90}h^{4}\\cdot\\frac{3}{\\sqrt{2\\pi}} = \\frac{h^{4}}{30\\sqrt{2\\pi}}.\n",
    "\\newline\n",
    "$$\n",
    "To ensure this is smaller than the indicated error of $10^{-5}$, we need $\\newline \\newline$\n",
    "$$\n",
    "\\frac{h^{4}}{30\\sqrt{2\\pi}} < 10^{-5},\n",
    "\\newline\n",
    "$$\n",
    "and solving this for $h$ gives $h < 0.16559\\dots$. Therefore, we can pick $h = 0.1$ and we'll be in the desired tolerance. Using this step size, Simpson's rule makes the approximation $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{y^{2}}{2}}dy &\\approx \\frac{0.1}{3}\\Big(f(-1) + 4f(-0.9) + 2f(-0.8) + 4f(-0.7) + 2f(-0.6) + \\dots + 2f(0.6) + 4f(0.7) + 2f(0.8) + 4f(0.9) + f(1)\\Big) \\\\\n",
    "    &\\approx 0.6826900.\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "This means that we are predicting that $68.269\\%$ of samples will be within a standard deviation of the mean. Let's see if that agrees with computing this integral through one of Julia's built-in packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuadGK\n",
    "\n",
    "f(x) = 1/âˆš(2*pi)*exp(-x^2/2)\n",
    "a = -1; b = 1\n",
    "quadgk(f, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QuadGK package returns the approximated value of the integral, and an estimate on the error bound. Here, our computed value is about $0.6286895$ accurate to within about $10^{-10}$, and we can see that our approximation is within $10^{-5}$ of that answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix of Calculations for Problem 4.4.16\n",
    "\n",
    "We need to maximize the fourth derivative of $\\newline \\newline$\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2\\sigma^{2}}x^{2}}.\n",
    "\\newline\n",
    "$$\n",
    "Repeated differentiation gives the first through fifth derivatives: $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f'(x) &= \\frac{-x}{\\sigma^{3}\\sqrt{2\\pi}}e^{-\\frac{1}{2\\sigma^{2}}x^{2}} \\\\\n",
    "    f''(x) &= \\Bigg(\\frac{-1}{\\sigma^{3}\\sqrt{2\\pi}} + \\frac{x^{2}}{\\sigma^{5}\\sqrt{2\\pi}}\\Bigg)e^{-\\frac{1}{2\\sigma^{2}}x^{2}} \\\\\n",
    "    f'''(x) &= \\Bigg(\\frac{3x}{\\sigma^{5}\\sqrt{2\\pi}} - \\frac{x^{3}}{\\sigma^{7}\\sqrt{2\\pi}}\\Bigg)e^{-\\frac{1}{2\\sigma^{2}}x^{2}} \\\\\n",
    "    f^{(4)}(x) &= \\Bigg(\\frac{3}{\\sigma^{5}\\sqrt{2\\pi}} - \\frac{6x^{2}}{\\sigma^{7}\\sqrt{2\\pi}} + \\frac{x^{4}}{\\sigma^{9}\\sqrt{2\\pi}}\\Bigg)e^{-\\frac{1}{2\\sigma^{2}}x^{2}} \\\\\n",
    "    f^{(5)}(x) &= \\Bigg(\\frac{-15x}{\\sigma^{7}\\sqrt{2\\pi}} + \\frac{10x^{3}}{\\sigma^{9}\\sqrt{2\\pi}} - \\frac{x^{5}}{\\sigma^{11}\\sqrt{2\\pi}}\\Bigg)e^{-\\frac{1}{2\\sigma^{2}}x^{2}}\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "To maximize the fourth derivative, we need to find where the fifth derivative equals zero. Since the exponential term is never zero, we find that the critical points satisfy $\\newline \\newline$\n",
    "$$\n",
    "x\\Big(x^{4} - 10\\sigma^{2}x^{2} + 15\\sigma^{4}\\Big) = 0,\n",
    "\\newline\n",
    "$$\n",
    "which means that $x = 0$ is a solution and there are four other solutions. An application of the first derivative test shows that $x = 0$ and two of the solutions to $x^{4} - 10\\sigma^{2}x^{2} + 15\\sigma^{4}$ are maxima of $f^{(4)}(x)$, and plugging in the numbers and using the fact that $\\sigma > 0$ tells us that $x = 0$ is the location of the global maximum of $f^{(4)}(x)$. Evaluating $f^{(4)}(0)$ gives $\\newline \\newline$\n",
    "$$\n",
    "f^{(4)}(0) = \\frac{3}{\\sigma^{5}\\sqrt{2\\pi}},\n",
    "$$\n",
    "which is the original $\\sigma$-dependent upper bound used in the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
