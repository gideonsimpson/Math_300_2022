{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH 300: Numerical Analysis I Recitation\n",
    "\n",
    "## Instructor: Liam Doherty\n",
    "\n",
    "## Meeting Time/Place: F 11:00AM, Curtis 453\n",
    "\n",
    "## Office Hours: MRC Hours MR 5p-7p, W 12p-2p, or in Korman 282 by appointment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.4.22(a)\n",
    "\n",
    "The equation $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-t^{2}/2}dt = 0.45\n",
    "\\newline\n",
    "$$\n",
    "can be solved for $x$ by using Newton's method with $\\newline \\newline$\n",
    "$$\n",
    "f(x) = \\int_{0}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-t^{2}/2}dt - 0.45\n",
    "\\newline\n",
    "$$\n",
    "and $\\newline \\newline$\n",
    "$$\n",
    "f'(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^{2}/2}.\n",
    "\\newline\n",
    "$$\n",
    "To evaluate $f$ at the approximation $p_{k}$, we need a quadrature formula to approximate $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0}^{p_{k}}\\frac{1}{\\sqrt{2\\pi}}e^{-t^{2}/2}dt.\n",
    "\\newline\n",
    "$$\n",
    "Find a solution to $f(x) = 0$ accurate to within $10^{-5}$ using Newton's method with $p_{0} = 0.5$ and the Composite Simpson's rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "A quick computation reveals that $x < 2$ (there is underlying intuition here about what the $Z$-score for a $90\\%$ confidence interval is):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuadGK\n",
    "\n",
    "f(x) = 1/√(2*pi)*exp(-x^2/2)\n",
    "a = 0; b = 2\n",
    "quadgk(f, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use $x = 2$ as an upper bound for our $b - a$ portion of our error term then, so that taking a step size of $h < 0.16559$ with Composite Simpson will be good enough so that our error does not get dominated by evaluation of the integral. (To fully justify the fact that we never get $p_{k} > 2$ for any $k$, we make an argument about the concavity: Notice that $\\newline \\newline$\n",
    "$$\n",
    "f''(x) = \\frac{-x}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^{2}} < 0\n",
    "\\newline\n",
    "$$\n",
    "whenever $x > 0$; this means that the tangent lines will all lie above the graph. Combining this with the fact that $f(p_{0}) < 0$ (and perhaps also $f'(x) > 0$), we can argue that Newton's method does not overshoot the root.) Now, all we have to do is apply Newton's method, which is $\\newline \\newline$\n",
    "$$\n",
    "p_{k + 1} = p_{k} - \\frac{f(p_{k})}{f'(p_{k})},\n",
    "\\newline\n",
    "$$\n",
    "or, writing it with our functions, $\\newline \\newline$\n",
    "$$\n",
    "p_{k + 1} = p_{k} - \\frac{\\int_{0}^{p_{k}}\\frac{1}{\\sqrt{2\\pi}}e^{-x^{2}/2}dx - 0.45}{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{p_{k}^{2}}{2}}}.\n",
    "\\newline\n",
    "$$\n",
    "This is going to be extraordinarily tedious to calculate by hand many times, so we will do one iteration in gory detail and the rest we will take for granted. For the integration, we use $h = 0.05$ and let the integrand of the integral term be denoted by $g$: $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p_{1} &= 0.5 - \\frac{\\int_{0}^{0.5}\\frac{1}{\\sqrt{2\\pi}}e^{-x^{2}/2}dx - 0.45}{\\frac{1}{\\sqrt{2\\pi}}e^{-(0.5)^{2}/2}} \\\\\n",
    "    &\\approx 0.5 - \\frac{\\frac{0.05}{3}(g(0) + 4g(0.05) + 2g(0.1) + 4g(0.15) + 2g(0.2) + 4g(0.25) + 2g(0.3) + 4g(0.35) + 2g(0.4) + 4g(0.45) + g(0.5)) - 0.45}{\\frac{1}{\\sqrt{2\\pi}}e^{-(0.5)^{2}/2}} \\\\\n",
    "    &\\approx 1.23435.\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "Now, for the next iteration, we would need to adapt the step size; Simpson's method requires a uniform mesh, so we would need to pick a step size that partitions $[0, 1.23435]$ into an integer number of subintervals. This could be accomplished by splitting the interval into $11$ subintervals each with length $0.123435$. However, this would make the computation fairly expensive to do by hand. We can obtain a few more iterates with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simpson(f::Function, a::Real, b::Real, n::Int)\n",
    "    @assert a < b \"Invalid interval!\"\n",
    "    h = (b - a)/n\n",
    "    interior_nodes = [a + i*h for i in 1:n-1]\n",
    "    \n",
    "    # Start the sum with left endpoint\n",
    "    sum = f(a)\n",
    "    \n",
    "    # Add in all interior nodes, accounting for even or odd index\n",
    "    for (j, node) in enumerate(interior_nodes)\n",
    "        if mod(j, 2) == 1\n",
    "            sum += 4*f(node)\n",
    "        elseif mod(j, 2) == 0\n",
    "            sum += 2*f(node)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Add in right endpoint and multiply by h/3\n",
    "    sum += f(b)\n",
    "    sum *= h/3\n",
    "    return sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton(f::Function, df::Function, p₀::Real; abs_tol = 1e-5, max_iters = 100, verbose = true)\n",
    "    converged = false\n",
    "    n = 0\n",
    "    \n",
    "    p = p₀\n",
    "    while !converged\n",
    "        n += 1\n",
    "        p_old = p\n",
    "        \n",
    "        @assert df(p) != 0 \"The derivative evaluated at the current point is zero! Cannot proceed. Last point: $(p)\"\n",
    "        \n",
    "        # Main step for algorithm\n",
    "        p = p - f(p)/df(p)\n",
    "        \n",
    "        # Status updates\n",
    "        if verbose\n",
    "            println(\"Old p: $(p_old), New p: $(p), Iteration: $(n)\")\n",
    "        end\n",
    "        \n",
    "        # Check convergence\n",
    "        if abs(p - p_old) < abs_tol\n",
    "            converged = true\n",
    "            return p\n",
    "        end\n",
    "        \n",
    "        # Return if algorithm does not converge\n",
    "        if n == max_iters\n",
    "            println(\"Did not converge in $(max_iters) iterations! Returning last value of p.\")\n",
    "            return p\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = 1/√(2*pi)*exp(-x^2/2)\n",
    "# Always using 10 subintervals for integration; this ensures that we will always be within 1e-5 in our approximation.\n",
    "f(x) = simpson(g, 0, x, 10) - 0.45; df(x) = g(x)\n",
    "newton(f, df, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only required 6 iterations to achieve the desired accuracy! Let's check that this agrees with a version of the code that uses packages to accomplish the integration and rootfinding tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Roots\n",
    "using QuadGK\n",
    "\n",
    "g(x) = 1/√(2*pi)*exp(-x^2/2)\n",
    "f(x) = quadgk(g, 0, x)[1] - 0.45; df(x) = g(x) # quadgk is NOT using Composite Simpson, but allows us to check answer\n",
    "\n",
    "find_zero((f, df), 0.5, Roots.Newton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Algorithms\n",
    "\n",
    "In a similar vein to the last problem, we can compose other algorithms that we have worked with together to solve problems. Consider some data generated by $\\sin(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = collect(0.:1.:2*pi)\n",
    "yvals = sin.(xvals)\n",
    "data = collect(zip(xvals, yvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to approximate the root of $\\sin(x)$ in $(0, 2\\pi)$ (notice the lack of inclusion). Given data of this form, we can construct an interpolant, using `DataInterpolations.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataInterpolations\n",
    "interpolant = LagrangeInterpolation(yvals, xvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the interpolant away from the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(interpolant(pi)) # Expect that this is near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plot(interpolant, xlabel = \"x\", ylabel = \"y\", label = \"Interpolant\")\n",
    "scatter!(xvals, sin.(xvals), label = \"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use an algorithm for rootfinding **on the interpolant** to approximate the root of the function which generated the original data. We can do that using `Roots.jl` with our interpolant constructed from `DataInterpolations.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Roots\n",
    "root = find_zero(interpolant, (xvals[2], xvals[end - 1])) # Don't include boundary points where roots are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the error from the true solution (which is of course $\\pi$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(root - pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Adding more points makes it even better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataInterpolations, Roots\n",
    "xvals = collect(0.:0.5:2*pi)\n",
    "yvals = sin.(xvals)\n",
    "interpolant = LagrangeInterpolation(yvals, xvals)\n",
    "root = find_zero(interpolant, (xvals[2], xvals[end - 1]))\n",
    "abs(root - pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that given data, we can also do rootfinding by first constructing an interpolant of the data (also called a **surrogate**) and then doing rootfinding with that function.\n",
    "\n",
    "I also want to demonstrate **Runge's Phenomenon**, which is something that can happen when doing Lagrange interpolation on certain types of data. You would expect that using more data points would lead to more accurate approximations, but surprisingly, this is not always the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "f(x) = 1/(1 + 25*x^2)\n",
    "xvals = collect(-1:0.5:1)\n",
    "yvals = f.(xvals)\n",
    "interpolant = LagrangeInterpolation(yvals, xvals)\n",
    "plot(interpolant, xlabel = \"x\", label = \"Interpolant\", legend = :outertopright,\n",
    "    title = \"Witch of Agnesi: Runge's Phenomenon\")\n",
    "scatter!(xvals, yvals, label = \"Data\")\n",
    "\n",
    "fine_mesh = LinRange(-1, 1, 101)\n",
    "plot!(fine_mesh, f.(fine_mesh), label = \"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look immediately alarming, but what if we increased the number of points in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 1/(1 + 25*x^2)\n",
    "xvals = collect(-1:0.1:1)\n",
    "yvals = f.(xvals)\n",
    "interpolant = LagrangeInterpolation(yvals, xvals)\n",
    "plot(interpolant, xlabel = \"x\", label = \"Interpolant\", legend = :outertopright,\n",
    "    title = \"Witch of Agnesi: Runge's Phenomenon\")\n",
    "scatter!(xvals, yvals, label = \"Data\")\n",
    "\n",
    "fine_mesh = LinRange(-1, 1, 101)\n",
    "plot!(fine_mesh, f.(fine_mesh), label = \"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the interpolant is behaving wildly near the ends of the interval. And it gets even worse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 1/(1 + 25*x^2)\n",
    "xvals = collect(-1:0.05:1)\n",
    "yvals = f.(xvals)\n",
    "interpolant = LagrangeInterpolation(yvals, xvals)\n",
    "plot(interpolant, xlabel = \"x\", label = \"Interpolant\", legend = :outertopright,\n",
    "    title = \"Witch of Agnesi: Runge's Phenomenon\")\n",
    "scatter!(xvals, yvals, label = \"Data\")\n",
    "\n",
    "fine_mesh = LinRange(-1, 1, 101)\n",
    "plot!(fine_mesh, f.(fine_mesh), label = \"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask: This looks bad! Does the error bound tell us that it could be this bad? The answer: Yes, of course it does! Recall the form of the error: $\\newline \\newline$\n",
    "$$\n",
    "f(x) - P(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod_{k = 0}^{n}(x - x_{k}).\n",
    "\\newline\n",
    "$$\n",
    "Really, if we are evaluating at a point, the thing that can go wrong is that the derivative might be excessively large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "df(x) = ForwardDiff.derivative(f, x)\n",
    "df(1 - 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TaylorSeries\n",
    "# Taylor expand f about the point 1\n",
    "t = taylor_expand(f, 1.; order = 100)\n",
    "println(t)\n",
    "t(0.) ≈ f(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a derivative of the Taylor expansion at 1 - 1e-3\n",
    "println(differentiate(t, 1)(-1e-3))\n",
    "differentiate(t, 1)(-1e-3) ≈ df(1 - 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some (scaled) maximums of derivatives of the function\n",
    "xvals = collect(-1:0.01:1)\n",
    "for n = 1:50\n",
    "    println(maximum(differentiate(t, n + 1).(xvals))/factorial(big(n + 1)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the term $\\newline \\newline$\n",
    "$$\n",
    "\\frac{1}{(n + 1)!}\\max_{\\xi \\in (-1, 1)}|f^{n + 1}(\\xi)|\n",
    "\\newline\n",
    "$$\n",
    "is blowing up as $n$ is getting larger. Thus, the error bounds are containing the information about Runge's Phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
